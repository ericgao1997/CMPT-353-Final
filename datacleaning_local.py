import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy import stats
import sys

# python3 datacleaning_local.py osm/amenities-vancouver.json.gz

### TAG SLICING ###

# manually categorized tags to more accurately identify node use
# check SCHEMA.txt, generated by spark's print_schema

# tags that indicate food or drink service to the general public
# food establishments are considered to be high-risk due to food handling and mask policy
TAGS_FOOD = [
  'amenity:cafe',
  'bar',
  'brewery',
  'cuisine',
  'fast_food',
  'food',
  'cocktails',
  'drink',
  'happy_hours',
  'microbrewery'
]

# tags that indicate an existing capacity for medical treatment
# note that this does NOT include buildings often converted to medical facilities
# such as schools, churches, and outdoor/tent facilities during crisis
TAGS_MEDICAL = [
  'emergency',
  'healthcare'
]

# tags that imply reasonable comemrcial activity or residential use
# my falsely flag business and residences that are closed to the public
TAGS_GATHERING = [
  'sport',
  'tourism',
  'service',
  'shop',
  'social_facility',
  'religion',
  'leisure',
  'memorial',
  'capacity',
  'kindergarten',
  'maxstay',
  'monastery:type',
  'outdoor_seating',
  'preschool',
  'shelter',
  'studio',
  'education',
  'office'
]

# tags that indicate consistent travel by vehicle and pedestrian
# expected to be well-documented, and thus the most complete data set
TAGS_TRANSPORTATION = [
  'bus',
  'car',
  'public_transport',
  'fuel',
  'ferry',
  'ferry_terminal'
]

# misc. tags that indicate a certain level of attention or notability
# nodes that are documented externally, have social media presence, or are indexed are more likely to be active
# the fact that these tags are added at all means the node is more likely to be active
TAGS_NOTABILITY = [
  'wikidata',
  'wikipedia:en',
  'yelp',
  'wlan',
  'addr:city',
  'alt',
  'alt_name',
  'seating',
  'seats'
]

def main(input_file):
  pd.options.display.max_columns = None
  pd.options.display.width = 200
  
  data = pd.read_json(input_file, lines=True)
  # json of form in SCHEMA.txt
  # https://wiki.openstreetmap.org/wiki/Tags
  
  # flatten json data into columns (135-ish)
  tags = pd.json_normalize(data['tags'])
  
  # output summary graphs of dataset (to sho how useless it is)
  data_summary(data)
    
  # affix aggregate category based on tags
  data['food'] = tags[TAGS_FOOD].isnull().all(axis=1) == False
  data['medical'] = tags[TAGS_MEDICAL].isnull().all(axis=1) == False
  data['gathering'] = tags[TAGS_GATHERING].isnull().all(axis=1) == False
  data['transport'] = tags[TAGS_TRANSPORTATION].isnull().all(axis=1) == False
  data['notable'] = tags[TAGS_NOTABILITY].isnull().all(axis=1) == False
  


  # column for number of user-specified tags (0 means untagged)
  data['tag_count'] = data.apply(lambda row: len(row['tags']), axis=1)
  
  # remove tag data
  data = data.drop(['tags'], axis=1)
  
  # count unique amenity types
  types = data.drop_duplicates(subset='amenity', keep="last")

  # output
  types['amenity'].to_csv("types.csv", index=False)
  data.to_csv("amenities-vancouver.csv", index=False)

# takes an input pandas dataframe and grates summary graphs
# used to illustrate the lack of usefulness of the data
def data_summary(data):
  
  return

if __name__=='__main__':
  input_file = sys.argv[1]
  main(input_file)         